# Don’t Get Prompted: A Lightweight Modular Pipeline for Detecting and Mitigating Prompt Injection in LLMs
![Python](https://img.shields.io/badge/Python-Compatible-green.svg)

## Abstract
Large Language Models (LLMs) now power search, productivity, and decision-support tools, but their open-ended interaction style exposes them to prompt injection and jailbreaking attacks. These threats manipulate model behavior, bypass safety filters, and leak sensitive data, raising technical, ethical, and societal risks. As LLMs are increasingly embedded in high-stakes applications, practical defenses against such adversarial prompts are vital for ensuring reliability, trust, and safe deployment. Prompt injection enables subtle, language-driven attacks that can override model intent, leak sensitive data, or bypass safety controls. As LLMs are increasingly embedded in user-facing and autonomous systems, defending against such manipulations is critical for ensuring trust and reliability. Recent work has introduced benchmarks to evaluate LLM robustness and explored detection strategies ranging from model-based self-evaluation to supervised classifiers. While these efforts provide valuable insights, many remain computationally intensive or narrowly scoped, limiting their applicability in real-world, dynamic environments. However, this LLM-as-a-judge approach inherits the same vulnerabilities it aims to mitigate, remaining susceptible to injection attacks and exhibiting inconsistent or biased judgments. Additionally, these methods often require running large models at inference time, making them ill-suited for cost-sensitive or resource-constrained deployments such as lightweight APIs. To address these open challenges, we present PromptGuard — a lightweight, multi-stage defense pipeline that semantically detects injected malicious instructions, filters known attack patterns, neutralizes stealthy injection tricks, and enforces strict input boundaries. This modular design eliminates reliance on large-model inference, offering a practical and efficient solution for securing LLM-powered systems in real-world deployments.
