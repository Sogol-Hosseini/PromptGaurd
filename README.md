# Don’t Get Prompted: A Lightweight Modular Pipeline for Detecting and Mitigating Prompt Injection in LLMs
![Python](https://img.shields.io/badge/Python-Compatible-green.svg)

## Abstract
Large language models now power search, productivity, and decision-support tools, but their open-ended interaction style exposes them to prompt injection and jailbreaking attacks. These threats manipulate model behavior, bypass safety filters, and leak sensitive data, raising technical, ethical, and societal risks. As LLMs are increasingly embedded in high-stakes applications, practical defenses against such adversarial prompts are vital for ensuring reliability, trust, and safe deployment. Prompt injection enables subtle, language-driven attacks that can override model intent, leak sensitive data, or bypass safety controls. As LLMs are increasingly embedded in user-facing and autonomous systems, defending against such manipulations is critical for ensuring trust and reliability. Prior research has thoroughly examined prompt injection and jailbreak attacks, proposing both semantic and rule-based defenses. However, most existing approaches rely on a single detection strategy, are computationally intensive, or tightly coupled to specific LLM architectures—limiting their practicality and generalizability. In this work, we introduce PromptGuard, a low-overhead hybrid pipeline that combines semantic analysis, pattern-based heuristics, and structural signals to detect both explicit injection attempts and subtle prompt manipulations. Unlike defenses that require model retraining or depend on LLMs for evaluation, PromptGuard operates externally and efficiently, making it suitable for deployment in real-world systems.
